{
    "docs": [
        {
            "location": "/", 
            "text": "Automatron is a framework for creating self-healing infrastructure. Simply put, it detects system events \n takes action to correct them.\n\n\nThe goal of Automatron is to allow users to automate the execution of common tasks performed during system events. These tasks can be as simple as \nsending an email\n to as complicated as \nrestarting services across multiple hosts\n.\n\n\n\n\nFeatures\n\n\n\n\nAutomatically detect and add new systems to monitor\n\n\nMonitoring is executed over SSH and completely \nagent-less\n\n\nPolicy based \nRunbooks\n allow for monitoring policies rather than server specific configurations\n\n\nSupports Nagios compliant health check scripts\n\n\nAllows dead simple \narbitrary shell commands\n for both \nchecks\n and \nactions\n\n\nRunbook flexibility with \nJinja2\n templating support\n\n\nPluggable Architecture that simplifies customization\n\n\nBootstrap based dashboard showing real-time events\n\n\n\n\nRunbooks\n\n\nThe core of Automatron is based around \nRunbooks\n. Runbooks are policies that define health checks and actions. You can think of them in the same way you would think of a printed runbook. Except with Automatron, the actions are automated.\n\n\nA simple Runbook example\n\n\nThe below runbook is a very basic example, it will check if NGINX is running (every 2 minutes) and restart it after 2 unsuccessful checks.\n\n\nname\n:\n \nCheck NGINX\n\n\nschedule\n:\n \n*/2\n \n*\n \n*\n \n*\n \n*\n\n\nchecks\n:\n\n  \nnginx_is_running\n:\n\n    \nexecute_from\n:\n \ntarget\n\n    \ntype\n:\n \ncmd\n\n    \ncmd\n:\n \nservice nginx status\n\n\nactions\n:\n\n  \nrestart_nginx\n:\n\n    \nexecute_from\n:\n \ntarget\n\n    \ntrigger\n:\n \n2\n\n    \nfrequency\n:\n \n300\n\n    \ncall_on\n:\n\n      \n-\n \nWARNING\n\n      \n-\n \nCRITICAL\n\n      \n-\n \nUNKNOWN\n\n    \ntype\n:\n \ncmd\n\n    \ncmd\n:\n \nservice nginx restart\n\n\n\n\n\n\nThe above actions will be performed every 300 seconds (5 minutes) until the health check returns an OK status. This delay allows time for NGINX to restart after each execution.\n\n\nA complex Runbook with Jinja2\n\n\nThis next runbook example is a more complex version of the above. In this example we will use Jinja2 and Automatron's Facts to enhance our runbook further.\n\n\nname\n:\n \nCheck NGINX\n\n\n{%\n \nif\n \nprod\n \nin\n \nfacts\n[\nhostname\n]\n \n%}\n\n\nschedule\n:\n\n  \nsecond\n:\n \n*\n/20\n\n\n{%\n \nelse\n \n%}\n\n\nschedule\n:\n \n*/2\n \n*\n \n*\n \n*\n \n*\n\n\n{%\n \nendif\n \n%}\n\n\nchecks\n:\n\n  \nnginx_is_running\n:\n\n    \nexecute_from\n:\n \ntarget\n\n    \ntype\n:\n \ncmd\n\n    \ncmd\n:\n \nservice nginx status\n\n\nactions\n:\n\n  \nrestart_nginx\n:\n\n    \nexecute_from\n:\n \ntarget\n\n    \ntrigger\n:\n \n2\n\n    \nfrequency\n:\n \n300\n\n    \ncall_on\n:\n\n      \n-\n \nWARNING\n\n      \n-\n \nCRITICAL\n\n      \n-\n \nUNKNOWN\n\n    \ntype\n:\n \ncmd\n\n    \ncmd\n:\n \nservice nginx restart\n\n  \nremove_from_dns\n:\n\n    \nexecute_from\n:\n \nremote\n\n    \ntrigger\n:\n \n0\n\n    \nfrequency\n:\n \n0\n\n    \ncall_on\n:\n\n      \n-\n \nWARNING\n\n      \n-\n \nCRITICAL\n\n      \n-\n \nUNKNOWN\n\n    \ntype\n:\n \nplugin\n\n    \nplugin\n:\n \ncloudflare/dns.py\n\n    \nargs\n:\n \nremove test@example.com apikey123 example.com --content\n \n{{\n \nfacts\n[\nnetwork\n][\neth0\n][\nv4\n][\n0\n]\n \n}}\n\n\n\n\n\n\nThe above example uses \nJinja2\n and \nFacts\n to create a conditional schedule. If our target server has a hostname that contains the word \"prod\" within it. The schedule for the health check will be every 20 seconds. If not, it will be every 2 minutes.\n\n\nAnother addition is the \nremove_from_dns\n action, which will remove the target server's DNS entry using the \nCloudFlare DNS\n plugin.\n\n\nBy using \nFacts\n and \nJinja2\n together you can customize a single runbook to cover unique actions for multiple hosts and environments.\n\n\nFollow Automatron", 
            "title": "Introduction"
        }, 
        {
            "location": "/#features", 
            "text": "Automatically detect and add new systems to monitor  Monitoring is executed over SSH and completely  agent-less  Policy based  Runbooks  allow for monitoring policies rather than server specific configurations  Supports Nagios compliant health check scripts  Allows dead simple  arbitrary shell commands  for both  checks  and  actions  Runbook flexibility with  Jinja2  templating support  Pluggable Architecture that simplifies customization  Bootstrap based dashboard showing real-time events", 
            "title": "Features"
        }, 
        {
            "location": "/#runbooks", 
            "text": "The core of Automatron is based around  Runbooks . Runbooks are policies that define health checks and actions. You can think of them in the same way you would think of a printed runbook. Except with Automatron, the actions are automated.", 
            "title": "Runbooks"
        }, 
        {
            "location": "/#a-simple-runbook-example", 
            "text": "The below runbook is a very basic example, it will check if NGINX is running (every 2 minutes) and restart it after 2 unsuccessful checks.  name :   Check NGINX  schedule :   */2   *   *   *   *  checks : \n   nginx_is_running : \n     execute_from :   target \n     type :   cmd \n     cmd :   service nginx status  actions : \n   restart_nginx : \n     execute_from :   target \n     trigger :   2 \n     frequency :   300 \n     call_on : \n       -   WARNING \n       -   CRITICAL \n       -   UNKNOWN \n     type :   cmd \n     cmd :   service nginx restart   The above actions will be performed every 300 seconds (5 minutes) until the health check returns an OK status. This delay allows time for NGINX to restart after each execution.", 
            "title": "A simple Runbook example"
        }, 
        {
            "location": "/#a-complex-runbook-with-jinja2", 
            "text": "This next runbook example is a more complex version of the above. In this example we will use Jinja2 and Automatron's Facts to enhance our runbook further.  name :   Check NGINX  {%   if   prod   in   facts [ hostname ]   %}  schedule : \n   second :   * /20  {%   else   %}  schedule :   */2   *   *   *   *  {%   endif   %}  checks : \n   nginx_is_running : \n     execute_from :   target \n     type :   cmd \n     cmd :   service nginx status  actions : \n   restart_nginx : \n     execute_from :   target \n     trigger :   2 \n     frequency :   300 \n     call_on : \n       -   WARNING \n       -   CRITICAL \n       -   UNKNOWN \n     type :   cmd \n     cmd :   service nginx restart \n   remove_from_dns : \n     execute_from :   remote \n     trigger :   0 \n     frequency :   0 \n     call_on : \n       -   WARNING \n       -   CRITICAL \n       -   UNKNOWN \n     type :   plugin \n     plugin :   cloudflare/dns.py \n     args :   remove test@example.com apikey123 example.com --content   {{   facts [ network ][ eth0 ][ v4 ][ 0 ]   }}   The above example uses  Jinja2  and  Facts  to create a conditional schedule. If our target server has a hostname that contains the word \"prod\" within it. The schedule for the health check will be every 20 seconds. If not, it will be every 2 minutes.  Another addition is the  remove_from_dns  action, which will remove the target server's DNS entry using the  CloudFlare DNS  plugin.  By using  Facts  and  Jinja2  together you can customize a single runbook to cover unique actions for multiple hosts and environments.", 
            "title": "A complex Runbook with Jinja2"
        }, 
        {
            "location": "/#follow-automatron", 
            "text": "", 
            "title": "Follow Automatron"
        }, 
        {
            "location": "/install/", 
            "text": "In this page you will be guided through a basic installation of Automatron. If you wish to deploy Automatron within a container, you can skip this guide and follow the \nDocker deployment\n instructions.\n\n\nBasic Installation\n\n\nCurrently, the simplest method of installing Automatron is by either cloning the \nGitHub Repository\n or \ndownloading\n a specific release and installing dependencies.\n\n\nThis guide will walk through cloning the GitHub repository and starting an Automatron instance.\n\n\nPrerequisites\n\n\nThe below list is a set of base requirements for installing and running an Automatron instance.\n\n\n\n\nPython 2.7 or higher\n\n\nPython-dev Package\n\n\nPip\n\n\nRedis\n\n\nnmap\n\n\ngit\n\n\nlibffi-dev\n\n\nlibssl-dev\n\n\nbuild-essential\n\n\n\n\nOn Ubuntu systems these can be installed with the following command.\n\n\n$ sudo apt-get install python2.7 python-dev \n\\\n\n                       python-pip redis-server \n\\\n\n                       nmap git libffi-dev \n\\\n\n                       build-essential libssl-dev\n\n\n\n\n\nOnce installed we can proceed to Automatron's installation\n\n\nClone from Github\n\n\nThe first installation step is to simply clone the current repository from GitHub using \ngit\n and change to the newly created directory.\n\n\n$ git clone https://github.com/madflojo/automatron.git\n$ \ncd\n automatron\n\n\n\n\n\nThis will place the latest \nmaster\n (production ready) branch into the \nautomatron\n directory.\n\n\nInstall required python modules\n\n\nThe second installation step is to install the required python modules using the \npip\n command.\n\n\n$ sudo pip install -r requirements.txt\n$ sudo pip install honcho\n\n\n\n\n\nWith the above two steps complete, we can now move to \nConfiguration\n.\n\n\nStarting Automatron\n\n\nIn order to start Automatron you can simply execute the command below.\n\n\n$ honcho start\n\n\n\n\n\nTo shut down Automatron you can use the \nkill\n command to send the \nSIGTERM\n signal to the running processes.\n\n\nDashboard\n\n\nTo view the Automatron dashboard simply open up \nhttp://\ninstance ip\n:8000\n in your favorite browser. As target nodes are identified and runbooks are executed, events will start to be reflected on the dashboard.", 
            "title": "Basic Installation"
        }, 
        {
            "location": "/install/#basic-installation", 
            "text": "Currently, the simplest method of installing Automatron is by either cloning the  GitHub Repository  or  downloading  a specific release and installing dependencies.  This guide will walk through cloning the GitHub repository and starting an Automatron instance.", 
            "title": "Basic Installation"
        }, 
        {
            "location": "/install/#prerequisites", 
            "text": "The below list is a set of base requirements for installing and running an Automatron instance.   Python 2.7 or higher  Python-dev Package  Pip  Redis  nmap  git  libffi-dev  libssl-dev  build-essential   On Ubuntu systems these can be installed with the following command.  $ sudo apt-get install python2.7 python-dev  \\ \n                       python-pip redis-server  \\ \n                       nmap git libffi-dev  \\ \n                       build-essential libssl-dev  Once installed we can proceed to Automatron's installation", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/install/#clone-from-github", 
            "text": "The first installation step is to simply clone the current repository from GitHub using  git  and change to the newly created directory.  $ git clone https://github.com/madflojo/automatron.git\n$  cd  automatron  This will place the latest  master  (production ready) branch into the  automatron  directory.", 
            "title": "Clone from Github"
        }, 
        {
            "location": "/install/#install-required-python-modules", 
            "text": "The second installation step is to install the required python modules using the  pip  command.  $ sudo pip install -r requirements.txt\n$ sudo pip install honcho  With the above two steps complete, we can now move to  Configuration .", 
            "title": "Install required python modules"
        }, 
        {
            "location": "/install/#starting-automatron", 
            "text": "In order to start Automatron you can simply execute the command below.  $ honcho start  To shut down Automatron you can use the  kill  command to send the  SIGTERM  signal to the running processes.", 
            "title": "Starting Automatron"
        }, 
        {
            "location": "/install/#dashboard", 
            "text": "To view the Automatron dashboard simply open up  http:// instance ip :8000  in your favorite browser. As target nodes are identified and runbooks are executed, events will start to be reflected on the dashboard.", 
            "title": "Dashboard"
        }, 
        {
            "location": "/install/docker/", 
            "text": "Deploying Automatron within Docker is quick and easy and can be done with two simple \ndocker\n commands.\n\n\nStarting a Redis container\n\n\nSince Automatron by default uses \nredis\n as a datastore we must first start a \nredis\n container.\n\n\n$ sudo docker run -d --restart\n=\nalways --name redis redis\n\n\n\n\n\nThe above \nredis\n instance will be used as a default datastore for Automatron.\n\n\nStarting the Automatron container\n\n\nOnce the \nredis\n instance is up and running we can start an Automatron instance.\n\n\n$ sudo docker run -d --link redis:redis -p \n8000\n:8000 -p \n9000\n:9000 -v /path/to/config:/config --restart\n=\nalways --name automatron madflojo/automatron\n\n\n\n\n\nIn the above \ndocker run\n command we are using \n-v\n to mount a directory from the host to the container as \n/config\n. This \n/config\n directory will be the home to Automatron's configuration files and Runbooks.\n\n\nDashboard\n\n\nTo view the Automatron dashboard simply open up \nhttp://\nhost ip\n:8000\n in your favorite browser. As target nodes are identified and runbooks are executed, events will start to be reflected on the dashboard.\n\n\nWith these steps complete, we can now move to \nConfiguring\n Automatron.\n\n\n\n\nTip\n\n\nA \ndocker-compose.yml\n file is included in the base repository which can be used to quickly stand up environments using \ndocker-compose up automatron\n.", 
            "title": "Deploy with Docker"
        }, 
        {
            "location": "/install/docker/#starting-a-redis-container", 
            "text": "Since Automatron by default uses  redis  as a datastore we must first start a  redis  container.  $ sudo docker run -d --restart = always --name redis redis  The above  redis  instance will be used as a default datastore for Automatron.", 
            "title": "Starting a Redis container"
        }, 
        {
            "location": "/install/docker/#starting-the-automatron-container", 
            "text": "Once the  redis  instance is up and running we can start an Automatron instance.  $ sudo docker run -d --link redis:redis -p  8000 :8000 -p  9000 :9000 -v /path/to/config:/config --restart = always --name automatron madflojo/automatron  In the above  docker run  command we are using  -v  to mount a directory from the host to the container as  /config . This  /config  directory will be the home to Automatron's configuration files and Runbooks.", 
            "title": "Starting the Automatron container"
        }, 
        {
            "location": "/install/docker/#dashboard", 
            "text": "To view the Automatron dashboard simply open up  http:// host ip :8000  in your favorite browser. As target nodes are identified and runbooks are executed, events will start to be reflected on the dashboard.  With these steps complete, we can now move to  Configuring  Automatron.   Tip  A  docker-compose.yml  file is included in the base repository which can be used to quickly stand up environments using  docker-compose up automatron .", 
            "title": "Dashboard"
        }, 
        {
            "location": "/configure/", 
            "text": "Configuration of Automatron is fairly simple and contained within a single file; \nconfig/config.yml\n.\n\n\nThis guide will walk through configuring a basic Automatron instance.\n\n\nCopying the \nconfig.yml.example\n file\n\n\nThe fastest method to configure Automatron is to start with the example configuration file \nconfig/config.yml.example\n. This configuration file contains basic default values which can be used in most implementations of Automatron. To use this file we can simply rename it to the default Automatron configuration file \nconfig/config.yml\n.\n\n\n$ cp config/config.yml.example config/config.yml\n\n\n\n\n\nOnce complete, we can now start customizing our configuration file.\n\n\nSSH Details\n\n\nAutomatron relies on SSH to perform both health checks and actions. Within \nconfig.yml\n there is an SSH section which will allow us to define the necessary SSH details such as; \nuser\n to authenticate as, a \ngateway\n or \"jump server\" for SSH connections and a Private SSH \nkey\n.\n\n\nssh\n:\n \n# SSH Configuration\n\n  \nuser\n:\n \nroot\n\n  \ngateway\n:\n \nFalse\n\n  \nkey\n:\n \n|\n\n        \n-----BEGIN RSA PRIVATE KEY-----\n\n        \nthis is an example\n\n        \n-----END RSA PRIVATE KEY-----\n\n\n\n\n\n\nIf the \ngateway\n setting is left as \nFalse\n Automatron will login to each host directly. To specify a \"jump server\" simply specify the DNS or IP address of the desired server.\n\n\n  \ngateway\n:\n \n10.0.0.1\n\n\n\n\n\n\n\n\nInfo\n\n\nAt this time Automatron does not support using sudo or other privilege escalation tools. Any checks or actions will be performed via the user privileges specified in \nuser\n.\n\n\n\n\nEnable Auto Discovery\n\n\nBy default, Automatron will listen on port \n9000\n for any HTTP requests. When an HTTP request is made to Automatron the IP will be captured and that server will then be identified as a monitoring target.\n\n\nThere are several plugins that enable other methods for host discovery, in this section we will also enable the \nroster\n discovery plugin. This configuration is within the \ndiscovery\n section of the \nconfig.yml\n file.\n\n\ndiscovery\n:\n\n  \nupload_path\n:\n \n/tmp/\n\n  \nvetting_interval\n:\n \n30\n\n  \nplugins\n:\n\n    \n# Web Service for HTTP PINGs\n\n    \nwebping\n:\n\n      \nip\n:\n \n0.0.0.0\n\n      \nport\n:\n \n9000\n\n\n\n\n\n\nTo enable the \nroster\n plugin we simply need to append the \nroster\n configuration within the \nplugins\n key.\n\n\ndiscovery\n:\n\n  \nupload_path\n:\n \n/tmp/\n\n  \nvetting_interval\n:\n \n30\n\n  \nplugins\n:\n\n    \n# Web Service for HTTP PINGs\n\n    \nwebping\n:\n\n      \nip\n:\n \n0.0.0.0\n\n      \nport\n:\n \n9000\n\n    \n# Roster Discovery\n\n    \nroster\n:\n\n      \nhosts\n:\n\n        \n-\n \n10.0.0.1\n\n\n\n\n\n\nEach plugin has unique configuration details, additional discovery plugins can be found in the \nAutomatron Plugins\n project.\n\n\nAt this point Automatron has been configured. We can now move on to creating our own \nRunbooks\n.", 
            "title": "Configuration"
        }, 
        {
            "location": "/configure/#copying-the-configymlexample-file", 
            "text": "The fastest method to configure Automatron is to start with the example configuration file  config/config.yml.example . This configuration file contains basic default values which can be used in most implementations of Automatron. To use this file we can simply rename it to the default Automatron configuration file  config/config.yml .  $ cp config/config.yml.example config/config.yml  Once complete, we can now start customizing our configuration file.", 
            "title": "Copying the config.yml.example file"
        }, 
        {
            "location": "/configure/#ssh-details", 
            "text": "Automatron relies on SSH to perform both health checks and actions. Within  config.yml  there is an SSH section which will allow us to define the necessary SSH details such as;  user  to authenticate as, a  gateway  or \"jump server\" for SSH connections and a Private SSH  key .  ssh :   # SSH Configuration \n   user :   root \n   gateway :   False \n   key :   | \n         -----BEGIN RSA PRIVATE KEY----- \n         this is an example \n         -----END RSA PRIVATE KEY-----   If the  gateway  setting is left as  False  Automatron will login to each host directly. To specify a \"jump server\" simply specify the DNS or IP address of the desired server.     gateway :   10.0.0.1    Info  At this time Automatron does not support using sudo or other privilege escalation tools. Any checks or actions will be performed via the user privileges specified in  user .", 
            "title": "SSH Details"
        }, 
        {
            "location": "/configure/#enable-auto-discovery", 
            "text": "By default, Automatron will listen on port  9000  for any HTTP requests. When an HTTP request is made to Automatron the IP will be captured and that server will then be identified as a monitoring target.  There are several plugins that enable other methods for host discovery, in this section we will also enable the  roster  discovery plugin. This configuration is within the  discovery  section of the  config.yml  file.  discovery : \n   upload_path :   /tmp/ \n   vetting_interval :   30 \n   plugins : \n     # Web Service for HTTP PINGs \n     webping : \n       ip :   0.0.0.0 \n       port :   9000   To enable the  roster  plugin we simply need to append the  roster  configuration within the  plugins  key.  discovery : \n   upload_path :   /tmp/ \n   vetting_interval :   30 \n   plugins : \n     # Web Service for HTTP PINGs \n     webping : \n       ip :   0.0.0.0 \n       port :   9000 \n     # Roster Discovery \n     roster : \n       hosts : \n         -   10.0.0.1   Each plugin has unique configuration details, additional discovery plugins can be found in the  Automatron Plugins  project.  At this point Automatron has been configured. We can now move on to creating our own  Runbooks .", 
            "title": "Enable Auto Discovery"
        }, 
        {
            "location": "/runbooks/", 
            "text": "The core of Automatron is based around \nRunbooks\n. Runbooks are policies that define health checks and actions. You can think of them in the same way you would think of a printed runbook. Except with Automatron, the actions are automated.\n\n\nBelow is a very simple Runbook example.\n\n\nname\n:\n \nCheck NGINX\n\n\nschedule\n:\n \n*/2\n \n*\n \n*\n \n*\n \n*\n\n\nchecks\n:\n\n  \nnginx_is_running\n:\n\n    \nexecute_from\n:\n \ntarget\n\n    \ntype\n:\n \ncmd\n\n    \ncmd\n:\n \nservice nginx status\n\n\nactions\n:\n\n  \nrestart_nginx\n:\n\n    \nexecute_from\n:\n \ntarget\n\n    \ntrigger\n:\n \n2\n\n    \nfrequency\n:\n \n300\n\n    \ncall_on\n:\n\n      \n-\n \nWARNING\n\n      \n-\n \nCRITICAL\n\n      \n-\n \nUNKNOWN\n\n    \ntype\n:\n \ncmd\n\n    \ncmd\n:\n \nservice nginx restart\n\n\n\n\n\n\nThis guide will walk through creating the above runbook as well as applying this runbook to all monitored hosts.\n\n\nCreating the Runbook YAML file\n\n\nBy default, Runbooks are specified within the \nconfig/runbooks\n directory. The runbook we will be creating is used to manage the NGINX service. We will want this runbook to be easy to find. An easy way to do that would be to create the runbook with a similar name as the service it manages. We can do so in one of two ways.\n\n\nWe can either create a file \nconfig/runbooks/nginx.yml\n or \nconfig/runbooks/nginx/init.yml\n. Either option are acceptable for the next steps. For this guide we will create the file as \nconfig/runbooks/nginx/init.yml\n.\n\n\n$ mkdir -p config/runbooks/nginx\n$ vi config/runbooks/nginx/init.yml\n\n\n\n\n\nTo get started let's go ahead and create the runbook by inserting our example runbook.\n\n\nname\n:\n \nCheck NGINX\n\n\nschedule\n:\n \n*/2\n \n*\n \n*\n \n*\n \n*\n\n\nchecks\n:\n\n  \nnginx_is_running\n:\n\n    \nexecute_from\n:\n \ntarget\n\n    \ntype\n:\n \ncmd\n\n    \ncmd\n:\n \nservice nginx status\n\n\nactions\n:\n\n  \nrestart_nginx\n:\n\n    \nexecute_from\n:\n \ntarget\n\n    \ntrigger\n:\n \n2\n\n    \nfrequency\n:\n \n300\n\n    \ncall_on\n:\n\n      \n-\n \nWARNING\n\n      \n-\n \nCRITICAL\n\n      \n-\n \nUNKNOWN\n\n    \ntype\n:\n \ncmd\n\n    \ncmd\n:\n \nservice nginx restart\n\n\n\n\n\n\nThe Anatomy of a Runbook\n\n\nA runbook consists of 4 major parameters; \nname\n, \nschedule\n, \nchecks\n, \n \nactions\n.\n\n\nName\n\n\nThe \nname\n field is used to provide an arbitrary name for the runbook. This field is a required field and must have some value. It is required that this value be unique and not re-used by other runbooks as this name will be referenced internally within Automatron.\n\n\nSchedule\n\n\nThe \nschedule\n field is used to provide a cron formatted schedule for health check execution. A cron formatted schedule of \n*/2 * * * *\n will result in the health checks being executed every 2 minutes.\n\n\n\n\nWarning\n\n\nDue to YAML formatting the cron schedule should be encased in single or double quotes such as \n'*/2 * * * *'\n. Failure to do so will result in a parsing error from YAML.\n\n\n\n\nAlternative schedule format\n\n\nIt is also possible to define a schedule in a key/value based cron format such as the example below.\n\n\nschedule\n:\n\n  \nsecond\n:\n \n*/15\n\n  \nminute\n:\n \n*\n\n  \nhour\n:\n \n*\n\n  \nday\n:\n \n*\n\n  \nmonth\n:\n \n*\n\n  \nday_of_week\n:\n \n*\n\n\n\n\n\n\nUsing this format you may omit keys that have a value of \n*\n as this is the default value. For example, the above schedule could also be represented as the below.\n\n\nschedule\n:\n\n  \nsecond\n:\n \n*/15\n\n\n\n\n\n\n\n\nWarning\n\n\nWhen using the key/value based format it is important to specify the \nsecond\n parameter, as a default value of \n*\n would result in checks being run every second.\n\n\n\n\nChecks\n\n\nThe \nchecks\n field is a YAML dictionary that contains the health checks to be executed against monitored hosts. The format of \nchecks\n is as follows.\n\n\nchecks\n:\n\n  \nname_of_check\n:\n\n    \n# health check options\n\n  \nanother_check\n:\n\n    \n# health check options\n\n\n\n\n\n\nFor more details around required health check parameters please read the \nChecks\n section.\n\n\nActions\n\n\nLike \nchecks\n, the \nactions\n field is a YAML dictionary that contains actions to be executed based on health check status. The \nactions\n field also follows a similar format to the \nchecks\n field.\n\n\nactions\n:\n\n  \nname_of_action\n:\n\n    \n# Action options\n\n  \nanother_action\n:\n\n    \n# Action options\n\n\n\n\n\n\nFor more details around required action parameters please read the \nActions\n section.\n\n\nApplying the Runbook\n\n\nBy creating the \nconfig/runbooks/nginx/init.yml\n we have only defined the runbook itself. This runbook however will not be applied to any monitored hosts until we specify which hosts it should be applied to.\n\n\nTo do this we will need to edit the \nconfig/runbooks/init.yml\n file. This file is a master list of any runbook to host mappings. To apply our runbook to all hosts we can simply insert the following into this file.\n\n\n*\n:\n\n  \n-\n \nnginx\n\n\n\n\n\n\nThe first field \n'*'\n is a Glob based matching used against the target hostname. In this case since the value is \n*\n, all hosts will be matched.\n\n\nIf we wished to limit this runbook to severs with naming scheme of \nweb001.example.com\n we could do so with the following modification.\n\n\nweb*\n:\n\n  \n-\n \nnginx\n\n\n\n\n\n\nSpecifying multiple targets and runbooks\n\n\nIt is possible to specify multiple host and runbook mappings such as the above. The below is an example of what an \nrunbooks/init.yml\n may look like for a environment hosting a two tier web application.\n\n\n*\n:\n\n  \n-\n \ncpu\n\n  \n-\n \nmem_free\n\n  \n-\n \ndisk_free\n\n  \n-\n \nntp\n\n  \n-\n \nssh\n\n\nweb*\n:\n\n  \n-\n \nnginx\n\n  \n-\n \nuwsgi\n\n\ndb*\n:\n\n  \n-\n \nmysql\n\n\n\n\n\n\nAt this point we have a basic runbook that is being applied to all hosts. To make these changes take effect, simply restart Automatron.", 
            "title": "Basics"
        }, 
        {
            "location": "/runbooks/#creating-the-runbook-yaml-file", 
            "text": "By default, Runbooks are specified within the  config/runbooks  directory. The runbook we will be creating is used to manage the NGINX service. We will want this runbook to be easy to find. An easy way to do that would be to create the runbook with a similar name as the service it manages. We can do so in one of two ways.  We can either create a file  config/runbooks/nginx.yml  or  config/runbooks/nginx/init.yml . Either option are acceptable for the next steps. For this guide we will create the file as  config/runbooks/nginx/init.yml .  $ mkdir -p config/runbooks/nginx\n$ vi config/runbooks/nginx/init.yml  To get started let's go ahead and create the runbook by inserting our example runbook.  name :   Check NGINX  schedule :   */2   *   *   *   *  checks : \n   nginx_is_running : \n     execute_from :   target \n     type :   cmd \n     cmd :   service nginx status  actions : \n   restart_nginx : \n     execute_from :   target \n     trigger :   2 \n     frequency :   300 \n     call_on : \n       -   WARNING \n       -   CRITICAL \n       -   UNKNOWN \n     type :   cmd \n     cmd :   service nginx restart", 
            "title": "Creating the Runbook YAML file"
        }, 
        {
            "location": "/runbooks/#the-anatomy-of-a-runbook", 
            "text": "A runbook consists of 4 major parameters;  name ,  schedule ,  checks ,    actions .", 
            "title": "The Anatomy of a Runbook"
        }, 
        {
            "location": "/runbooks/#name", 
            "text": "The  name  field is used to provide an arbitrary name for the runbook. This field is a required field and must have some value. It is required that this value be unique and not re-used by other runbooks as this name will be referenced internally within Automatron.", 
            "title": "Name"
        }, 
        {
            "location": "/runbooks/#schedule", 
            "text": "The  schedule  field is used to provide a cron formatted schedule for health check execution. A cron formatted schedule of  */2 * * * *  will result in the health checks being executed every 2 minutes.   Warning  Due to YAML formatting the cron schedule should be encased in single or double quotes such as  '*/2 * * * *' . Failure to do so will result in a parsing error from YAML.", 
            "title": "Schedule"
        }, 
        {
            "location": "/runbooks/#alternative-schedule-format", 
            "text": "It is also possible to define a schedule in a key/value based cron format such as the example below.  schedule : \n   second :   */15 \n   minute :   * \n   hour :   * \n   day :   * \n   month :   * \n   day_of_week :   *   Using this format you may omit keys that have a value of  *  as this is the default value. For example, the above schedule could also be represented as the below.  schedule : \n   second :   */15    Warning  When using the key/value based format it is important to specify the  second  parameter, as a default value of  *  would result in checks being run every second.", 
            "title": "Alternative schedule format"
        }, 
        {
            "location": "/runbooks/#checks", 
            "text": "The  checks  field is a YAML dictionary that contains the health checks to be executed against monitored hosts. The format of  checks  is as follows.  checks : \n   name_of_check : \n     # health check options \n   another_check : \n     # health check options   For more details around required health check parameters please read the  Checks  section.", 
            "title": "Checks"
        }, 
        {
            "location": "/runbooks/#actions", 
            "text": "Like  checks , the  actions  field is a YAML dictionary that contains actions to be executed based on health check status. The  actions  field also follows a similar format to the  checks  field.  actions : \n   name_of_action : \n     # Action options \n   another_action : \n     # Action options   For more details around required action parameters please read the  Actions  section.", 
            "title": "Actions"
        }, 
        {
            "location": "/runbooks/#applying-the-runbook", 
            "text": "By creating the  config/runbooks/nginx/init.yml  we have only defined the runbook itself. This runbook however will not be applied to any monitored hosts until we specify which hosts it should be applied to.  To do this we will need to edit the  config/runbooks/init.yml  file. This file is a master list of any runbook to host mappings. To apply our runbook to all hosts we can simply insert the following into this file.  * : \n   -   nginx   The first field  '*'  is a Glob based matching used against the target hostname. In this case since the value is  * , all hosts will be matched.  If we wished to limit this runbook to severs with naming scheme of  web001.example.com  we could do so with the following modification.  web* : \n   -   nginx", 
            "title": "Applying the Runbook"
        }, 
        {
            "location": "/runbooks/#specifying-multiple-targets-and-runbooks", 
            "text": "It is possible to specify multiple host and runbook mappings such as the above. The below is an example of what an  runbooks/init.yml  may look like for a environment hosting a two tier web application.  * : \n   -   cpu \n   -   mem_free \n   -   disk_free \n   -   ntp \n   -   ssh  web* : \n   -   nginx \n   -   uwsgi  db* : \n   -   mysql   At this point we have a basic runbook that is being applied to all hosts. To make these changes take effect, simply restart Automatron.", 
            "title": "Specifying multiple targets and runbooks"
        }, 
        {
            "location": "/runbooks/checks/", 
            "text": "Automatron determines whether a runbook action should be performed based on the results of a health check. There are two types of health checks within Automatron.  \nArbitrary shell commands\n and \nPlugin executables\n. In this guide we will walk through defining two health checks, one of each type.\n\n\nThe below runbook is a sample that this guide will be based on.\n\n\nname\n:\n \nCheck NGINX\n\n\nschedule\n:\n \n*/2\n \n*\n \n*\n \n*\n \n*\n\n\nchecks\n:\n\n  \nnginx_is_running\n:\n\n    \nexecute_from\n:\n \ntarget\n\n    \ntype\n:\n \ncmd\n\n    \ncmd\n:\n \nservice nginx status\n\n  \nport_443_is_up\n:\n\n    \nexecute_from\n:\n \ntarget\n\n    \ntype\n:\n \nplugin\n\n    \nplugin\n:\n \nnetwork/tcp_connect.py\n\n    \nargs\n:\n \n--host=localhost --port 443\n\n\nactions\n:\n\n  \nrestart_nginx\n:\n\n    \nexecute_from\n:\n \ntarget\n\n    \ntrigger\n:\n \n2\n\n    \nfrequency\n:\n \n300\n\n    \ncall_on\n:\n\n      \n-\n \nWARNING\n\n      \n-\n \nCRITICAL\n\n      \n-\n \nUNKNOWN\n\n    \ntype\n:\n \ncmd\n\n    \ncmd\n:\n \nservice nginx restart\n\n  \nremove_from_dns\n:\n\n    \nexecute_from\n:\n \nremote\n\n    \ntrigger\n:\n \n0\n\n    \nfrequency\n:\n \n0\n\n    \ncall_on\n:\n\n      \n-\n \nWARNING\n\n      \n-\n \nCRITICAL\n\n      \n-\n \nUNKNOWN\n\n    \ntype\n:\n \nplugin\n\n    \nplugin\n:\n \ncloudflare/dns.py\n\n    \nargs\n:\n \nremove test@example.com apikey123 example.com --content 10.0.0.1\n\n\n\n\n\n\nIn the above example, there are two health checks defined \nnginx_is_running\n and \nport_443_is_up\n. In the below section we will break down each of these health checks to better understand how health checks are defined.\n\n\nA command based health check\n\n\nCommand based health checks are one of the simplest concepts in Automatron. This type of health check allows users to define a command that is executed to determine the health status of a target.\n\n\nThis is accomplished by Automatron simply logging into the target system over SSH and executing the defined command. The exit code of the executed command is then used to determine the status of the health check.\n\n\nThe below sample is the \nnginx_is_running\n command based health check.\n\n\n  \nnginx_is_running\n:\n\n    \nexecute_from\n:\n \ntarget\n\n    \ntype\n:\n \ncmd\n\n    \ncmd\n:\n \nservice nginx status\n\n\n\n\n\n\nIn this sample we can see that there are 3 values required for command based health checks. Those values are \nexecute_from\n, \ntype\n, and \ncmd\n. Let's go ahead and break down these values to gain a better understanding of what they mean and tell Automatron to do.\n\n\nExecute from\n\n\nThe \nexecute_from\n field is used to specify where to run the health check. Acceptable values for this field are \ntarget\n which is used to execute the health check on the monitored node itself and \nremote\n. The \nremote\n setting will tell Automatron to execute the health check from the system running Automatron itself.\n\n\nIn our case the command we wish to execute can only be executed from the monitored system itself, as such the value of this field will be \ntarget\n.\n\n\nType\n\n\nThe \ntype\n field is used to specify what type of health check this check is. Acceptable values are \ncmd\n or \nplugin\n. In this case, since we are defining a command based health check our value is set to \ncmd\n.\n\n\nCommand\n\n\nThe \ncmd\n field is used to specify the shell command to execute. In our example the command is simply \nservice nginx status\n. However, this field can support much more complicated commands such as the below example.\n\n\ncmd\n:\n \n/usr/bin/curl -Lw \nResponse %{http_code}\\\\n\n http://10.0.0.1 -o /dev/null | egrep \nResponse [200|301]\n\n\n\n\n\n\nIt is not uncommon to use multiple commands connected with output redirection and conditionals within a runbook.\n\n\nA plugin based health check\n\n\nPlugin based health checks are similar to Command Based health checks in that the exit code is used to determine status. Where these checks differ is that Automatron will copy an executable to the target system and then execute that executable with the specified arguments.\n\n\nBelow is an example Plugin health check.\n\n\nport_443_is_up\n:\n\n  \nexecute_from\n:\n \ntarget\n\n  \ntype\n:\n \nplugin\n\n  \nplugin\n:\n \nnetwork/tcp_connect.py\n\n  \nargs\n:\n \n--host=localhost --port 443\n\n\n\n\n\n\nPlugin type health checks have 4 configuration items \nexecute_from\n, \ntype\n, \nplugin\n \n \nargs\n. Let's go ahead and break down these values to gain a better understanding of what they mean and tell Automatron to do.\n\n\nExecute from \n Type\n\n\nThe \nexecute_from\n and \ntype\n fields are common fields for every runbook. The way they are applied for plugin health checks is the same as the way they are applied for command based health checks. As such we will skip repeating these fields in this section.\n\n\nPlugin\n\n\nThe \nplugin\n field is used to specify the location of the plugin executable. This is a relative file path starting from the value of the \nplugin_path\n parameter located within the \nconfig/config.yml\n configuration file.\n\n\nFor example, a plugin located at \n/path/to/plugins/checks/mycheck/mycheck.pl\n would require the value of \nmycheck/mycheck.pl\n.\n\n\nPlugin Arguments\n\n\nThe \nargs\n field is used to specify the arguments to provide the plugin executable. In the example above the plugin will be executed as follows by Automatron\n\n\n$ /path/to/plugins/checks/network/tcp_connect.py --host\n=\nlocalhost --port \n443\n\n\n\n\n\n\nUsing Exit Codes to relay health check status\n\n\nAutomatron follows the \nNagios\n model for health check exit codes. When a health check is executed the exit code is used to inform Automatron of the results. The below list is a map of acceptable exit codes and how they relate to Automatron health check status.\n\n\n\n\nOK\n: Requires a successful exit code of \n0\n\n\nWARNING\n: Is indicated by an exit code of \n1\n\n\nCRITICAL\n: Is indicated by an exit code of \n2\n\n\nUNKNOWN\n: Is indicated by any other exit code\n\n\n\n\n\n\nTip\n\n\nSince Automatron supports the \nNagios\n exit code strategy most Nagios compliant health checks can also be used with Automatron.", 
            "title": "Checks"
        }, 
        {
            "location": "/runbooks/checks/#a-command-based-health-check", 
            "text": "Command based health checks are one of the simplest concepts in Automatron. This type of health check allows users to define a command that is executed to determine the health status of a target.  This is accomplished by Automatron simply logging into the target system over SSH and executing the defined command. The exit code of the executed command is then used to determine the status of the health check.  The below sample is the  nginx_is_running  command based health check.     nginx_is_running : \n     execute_from :   target \n     type :   cmd \n     cmd :   service nginx status   In this sample we can see that there are 3 values required for command based health checks. Those values are  execute_from ,  type , and  cmd . Let's go ahead and break down these values to gain a better understanding of what they mean and tell Automatron to do.", 
            "title": "A command based health check"
        }, 
        {
            "location": "/runbooks/checks/#execute-from", 
            "text": "The  execute_from  field is used to specify where to run the health check. Acceptable values for this field are  target  which is used to execute the health check on the monitored node itself and  remote . The  remote  setting will tell Automatron to execute the health check from the system running Automatron itself.  In our case the command we wish to execute can only be executed from the monitored system itself, as such the value of this field will be  target .", 
            "title": "Execute from"
        }, 
        {
            "location": "/runbooks/checks/#type", 
            "text": "The  type  field is used to specify what type of health check this check is. Acceptable values are  cmd  or  plugin . In this case, since we are defining a command based health check our value is set to  cmd .", 
            "title": "Type"
        }, 
        {
            "location": "/runbooks/checks/#command", 
            "text": "The  cmd  field is used to specify the shell command to execute. In our example the command is simply  service nginx status . However, this field can support much more complicated commands such as the below example.  cmd :   /usr/bin/curl -Lw  Response %{http_code}\\\\n  http://10.0.0.1 -o /dev/null | egrep  Response [200|301]   It is not uncommon to use multiple commands connected with output redirection and conditionals within a runbook.", 
            "title": "Command"
        }, 
        {
            "location": "/runbooks/checks/#a-plugin-based-health-check", 
            "text": "Plugin based health checks are similar to Command Based health checks in that the exit code is used to determine status. Where these checks differ is that Automatron will copy an executable to the target system and then execute that executable with the specified arguments.  Below is an example Plugin health check.  port_443_is_up : \n   execute_from :   target \n   type :   plugin \n   plugin :   network/tcp_connect.py \n   args :   --host=localhost --port 443   Plugin type health checks have 4 configuration items  execute_from ,  type ,  plugin     args . Let's go ahead and break down these values to gain a better understanding of what they mean and tell Automatron to do.", 
            "title": "A plugin based health check"
        }, 
        {
            "location": "/runbooks/checks/#execute-from-type", 
            "text": "The  execute_from  and  type  fields are common fields for every runbook. The way they are applied for plugin health checks is the same as the way they are applied for command based health checks. As such we will skip repeating these fields in this section.", 
            "title": "Execute from &amp; Type"
        }, 
        {
            "location": "/runbooks/checks/#plugin", 
            "text": "The  plugin  field is used to specify the location of the plugin executable. This is a relative file path starting from the value of the  plugin_path  parameter located within the  config/config.yml  configuration file.  For example, a plugin located at  /path/to/plugins/checks/mycheck/mycheck.pl  would require the value of  mycheck/mycheck.pl .", 
            "title": "Plugin"
        }, 
        {
            "location": "/runbooks/checks/#plugin-arguments", 
            "text": "The  args  field is used to specify the arguments to provide the plugin executable. In the example above the plugin will be executed as follows by Automatron  $ /path/to/plugins/checks/network/tcp_connect.py --host = localhost --port  443", 
            "title": "Plugin Arguments"
        }, 
        {
            "location": "/runbooks/checks/#using-exit-codes-to-relay-health-check-status", 
            "text": "Automatron follows the  Nagios  model for health check exit codes. When a health check is executed the exit code is used to inform Automatron of the results. The below list is a map of acceptable exit codes and how they relate to Automatron health check status.   OK : Requires a successful exit code of  0  WARNING : Is indicated by an exit code of  1  CRITICAL : Is indicated by an exit code of  2  UNKNOWN : Is indicated by any other exit code    Tip  Since Automatron supports the  Nagios  exit code strategy most Nagios compliant health checks can also be used with Automatron.", 
            "title": "Using Exit Codes to relay health check status"
        }, 
        {
            "location": "/runbooks/actions/", 
            "text": "When a runbook health check returns a state; Automatron will check the runbooks definition to determine if an action should be taken. Like health checks, actions come in two flavors. \nArbitrary shell commands\n and \nPlugin executables\n. In this guide we will be defining two actions, one of each type.\n\n\nDuring this guide we will be building runbook actions for the below runbook.\n\n\nname\n:\n \nCheck NGINX\n\n\nschedule\n:\n \n*/2\n \n*\n \n*\n \n*\n \n*\n\n\nchecks\n:\n\n  \nnginx_is_running\n:\n\n    \nexecute_from\n:\n \ntarget\n\n    \ntype\n:\n \ncmd\n\n    \ncmd\n:\n \nservice nginx status\n\n  \nport_443_is_up\n:\n\n    \nexecute_from\n:\n \ntarget\n\n    \ntype\n:\n \nplugin\n\n    \nplugin\n:\n \nnetwork/tcp_connect.py\n\n    \nargs\n:\n \n--host=localhost --port 443\n\n\nactions\n:\n\n  \nrestart_nginx\n:\n\n    \nexecute_from\n:\n \ntarget\n\n    \ntrigger\n:\n \n2\n\n    \nfrequency\n:\n \n300\n\n    \ncall_on\n:\n\n      \n-\n \nWARNING\n\n      \n-\n \nCRITICAL\n\n      \n-\n \nUNKNOWN\n\n    \ntype\n:\n \ncmd\n\n    \ncmd\n:\n \nservice nginx restart\n\n  \nremove_from_dns\n:\n\n    \nexecute_from\n:\n \nremote\n\n    \ntrigger\n:\n \n0\n\n    \nfrequency\n:\n \n0\n\n    \ncall_on\n:\n\n      \n-\n \nWARNING\n\n      \n-\n \nCRITICAL\n\n      \n-\n \nUNKNOWN\n\n    \ntype\n:\n \nplugin\n\n    \nplugin\n:\n \ncloudflare/dns.py\n\n    \nargs\n:\n \nremove test@example.com apikey123 example.com --content 10.0.0.1\n\n\n\n\n\n\nWithin this runbook there are two actions; \nrestart_nginx\n and \nremove_from_dns\n. In this guide we will be breaking down these two actions to gain a better understanding of how they work.\n\n\nA command based actions\n\n\nLike health checks, Automatron actions also support arbitrary shell commands. When executing this type of action Automatron simply logs into the target system and executes the defined command.\n\n\nThe below example is a simple action that logs into the target system and executes the \nservice nginx restart\n command.\n\n\nrestart_nginx\n:\n\n \nexecute_from\n:\n \ntarget\n\n \ntrigger\n:\n \n2\n\n \nfrequency\n:\n \n300\n\n \ncall_on\n:\n\n   \n-\n \nWARNING\n\n   \n-\n \nCRITICAL\n\n   \n-\n \nUNKNOWN\n\n \ntype\n:\n \ncmd\n\n \ncmd\n:\n \nservice nginx restart\n\n\n\n\n\n\nThis action has 6 main fields defined; \nexecute_from\n, \ntrigger\n, \nfrequency\n, \ncall_on\n, \ntype\n, and \ncmd\n. Let's break down what each of these fields specify and control about action execution.\n\n\nExecute from\n\n\nThe \nexecute_from\n field is used to specify where to run the action. Acceptable values for this field are \ntarget\n, \nremote\n and \nhost\n.\n\n\n\n\ntarget\n - This value will specify that the action should be executed on the monitored host.\n\n\nremote\n - This value will specify that the action is executed from the Automatron server.\n\n\nhost\n - This value will specify that the action is executed from another specified host.\n\n\n\n\nWhen using the \nhost\n value, the alternative host must be specified via a key named \nhost\n. Below is an example of a \nhost\n based action.\n\n\nactions\n:\n\n  \nrestart_mysql\n:\n\n    \nexecute_from\n:\n \nhost\n\n    \nhost\n:\n \n10.0.0.2\n\n    \ntrigger\n:\n \n0\n\n    \nfrequency\n:\n \n300\n\n    \ncall_on\n:\n\n      \n-\n \nWARNING\n\n      \n-\n \nCRITICAL\n\n    \ntype\n:\n \ncmd\n\n    \ncmd\n:\n \nservice mysql restart\n\n\n\n\n\n\nThe above action will result in Automatron logging into \n10.0.0.2\n and executing \nservice mysql restart\n.\n\n\nTrigger\n\n\nThe \ntrigger\n field is used to specify the number of times a health check returns the state specified within \ncall_on\n. This number \nmust be reached consecutively\n. If for example, the health check returns \nWARNING\n and then \nOK\n; Automatron's internal counter will be reset.\n\n\nFrequecy\n\n\nThe \nfrequency\n field is used to specify the time (in seconds) between action execution. In the above example the action will be executed every \n300\n seconds until either the \ncall_on\n or \ntrigger\n conditions are no longer met.\n\n\nIf you wish to execute an action every time, simply set this value to \n0\n seconds.\n\n\nCall on\n\n\nThe \ncall_on\n field is a YAML list which is used to list the states that should trigger this action. Valid options are \nOK\n, \nWARNING\n, \nCRITICAL\n \n \nUNKNOWN\n.\n\n\nType\n\n\nThe \ntype\n field is used to specify what type of action will be performed. Acceptable values are \ncmd\n or \nplugin\n. This field is required for all actions. Since our action above is a command based action we will specify \ncmd\n.\n\n\nCommand\n\n\nThe \ncmd\n field is used to specify the shell command to execute as part of this action. In the example above the command is simply \nservice nginx restart\n. When this action is executed, Automatron will login to the host specified and execute that command.\n\n\nA plugin based action\n\n\nWhen a command based action is being executed Automatron will login to the target host and execute the command specified. With plugin based actions, Automatron will upload the plugin executable and execute it giving the specified arguments.\n\n\nBelow is a sample Runbook using a plugin based action.\n\n\nremove_from_dns\n:\n\n  \nexecute_from\n:\n \nremote\n\n  \ntrigger\n:\n \n0\n\n  \nfrequency\n:\n \n0\n\n  \ncall_on\n:\n\n    \n-\n \nWARNING\n\n    \n-\n \nCRITICAL\n\n    \n-\n \nUNKNOWN\n\n  \ntype\n:\n \nplugin\n\n  \nplugin\n:\n \ncloudflare/dns.py\n\n  \nargs\n:\n \nremove test@example.com apikey123 example.com --content 10.0.0.1\n\n\n\n\n\n\nThis action has 7 main fields defined; \nexecute_from\n, \ntrigger\n, \nfrequency\n, \ncall_on\n, \ntype\n, \nplugin\n and \nargs\n. Let's break down what each of these fields specify and control about action execution.\n\n\nExecute from, Trigger, Frequency, Call on \n Type\n\n\nAs \nexecute_from\n, \ntrigger\n, \nfrequency\n, \ncall_on\n, and \ntype\n are common fields for every runbook. The way they are applied for plugin actions is the same as the way they are applied for command based actions. As such we will skip repeating these fields in this section.\n\n\nPlugin\n\n\nThe \nplugin\n field is used to specify the location of the plugin executable. This is a relative file path starting from the value of the \nplugin_path\n parameter located within the \nconfig/config.yml\n configuration file.\n\n\nFor example, a plugin located at \n/path/to/plugins/actions/myaction/myaction.pl\n would require the value of \nmyaction/myaction.pl\n.\n\n\nPlugin Arguments\n\n\nThe \nargs\n field is used to specify the arguments to provide the plugin executable. In the example above the plugin will be executed as follows by Automatron\n\n\n$ /path/to/plugins/checks/cloudflare/dns.py remove test@example.com apikey123 example.com --content \n10\n.0.0.1", 
            "title": "Actions"
        }, 
        {
            "location": "/runbooks/actions/#a-command-based-actions", 
            "text": "Like health checks, Automatron actions also support arbitrary shell commands. When executing this type of action Automatron simply logs into the target system and executes the defined command.  The below example is a simple action that logs into the target system and executes the  service nginx restart  command.  restart_nginx : \n  execute_from :   target \n  trigger :   2 \n  frequency :   300 \n  call_on : \n    -   WARNING \n    -   CRITICAL \n    -   UNKNOWN \n  type :   cmd \n  cmd :   service nginx restart   This action has 6 main fields defined;  execute_from ,  trigger ,  frequency ,  call_on ,  type , and  cmd . Let's break down what each of these fields specify and control about action execution.", 
            "title": "A command based actions"
        }, 
        {
            "location": "/runbooks/actions/#execute-from", 
            "text": "The  execute_from  field is used to specify where to run the action. Acceptable values for this field are  target ,  remote  and  host .   target  - This value will specify that the action should be executed on the monitored host.  remote  - This value will specify that the action is executed from the Automatron server.  host  - This value will specify that the action is executed from another specified host.   When using the  host  value, the alternative host must be specified via a key named  host . Below is an example of a  host  based action.  actions : \n   restart_mysql : \n     execute_from :   host \n     host :   10.0.0.2 \n     trigger :   0 \n     frequency :   300 \n     call_on : \n       -   WARNING \n       -   CRITICAL \n     type :   cmd \n     cmd :   service mysql restart   The above action will result in Automatron logging into  10.0.0.2  and executing  service mysql restart .", 
            "title": "Execute from"
        }, 
        {
            "location": "/runbooks/actions/#trigger", 
            "text": "The  trigger  field is used to specify the number of times a health check returns the state specified within  call_on . This number  must be reached consecutively . If for example, the health check returns  WARNING  and then  OK ; Automatron's internal counter will be reset.", 
            "title": "Trigger"
        }, 
        {
            "location": "/runbooks/actions/#frequecy", 
            "text": "The  frequency  field is used to specify the time (in seconds) between action execution. In the above example the action will be executed every  300  seconds until either the  call_on  or  trigger  conditions are no longer met.  If you wish to execute an action every time, simply set this value to  0  seconds.", 
            "title": "Frequecy"
        }, 
        {
            "location": "/runbooks/actions/#call-on", 
            "text": "The  call_on  field is a YAML list which is used to list the states that should trigger this action. Valid options are  OK ,  WARNING ,  CRITICAL     UNKNOWN .", 
            "title": "Call on"
        }, 
        {
            "location": "/runbooks/actions/#type", 
            "text": "The  type  field is used to specify what type of action will be performed. Acceptable values are  cmd  or  plugin . This field is required for all actions. Since our action above is a command based action we will specify  cmd .", 
            "title": "Type"
        }, 
        {
            "location": "/runbooks/actions/#command", 
            "text": "The  cmd  field is used to specify the shell command to execute as part of this action. In the example above the command is simply  service nginx restart . When this action is executed, Automatron will login to the host specified and execute that command.", 
            "title": "Command"
        }, 
        {
            "location": "/runbooks/actions/#a-plugin-based-action", 
            "text": "When a command based action is being executed Automatron will login to the target host and execute the command specified. With plugin based actions, Automatron will upload the plugin executable and execute it giving the specified arguments.  Below is a sample Runbook using a plugin based action.  remove_from_dns : \n   execute_from :   remote \n   trigger :   0 \n   frequency :   0 \n   call_on : \n     -   WARNING \n     -   CRITICAL \n     -   UNKNOWN \n   type :   plugin \n   plugin :   cloudflare/dns.py \n   args :   remove test@example.com apikey123 example.com --content 10.0.0.1   This action has 7 main fields defined;  execute_from ,  trigger ,  frequency ,  call_on ,  type ,  plugin  and  args . Let's break down what each of these fields specify and control about action execution.", 
            "title": "A plugin based action"
        }, 
        {
            "location": "/runbooks/actions/#execute-from-trigger-frequency-call-on-type", 
            "text": "As  execute_from ,  trigger ,  frequency ,  call_on , and  type  are common fields for every runbook. The way they are applied for plugin actions is the same as the way they are applied for command based actions. As such we will skip repeating these fields in this section.", 
            "title": "Execute from, Trigger, Frequency, Call on &amp; Type"
        }, 
        {
            "location": "/runbooks/actions/#plugin", 
            "text": "The  plugin  field is used to specify the location of the plugin executable. This is a relative file path starting from the value of the  plugin_path  parameter located within the  config/config.yml  configuration file.  For example, a plugin located at  /path/to/plugins/actions/myaction/myaction.pl  would require the value of  myaction/myaction.pl .", 
            "title": "Plugin"
        }, 
        {
            "location": "/runbooks/actions/#plugin-arguments", 
            "text": "The  args  field is used to specify the arguments to provide the plugin executable. In the example above the plugin will be executed as follows by Automatron  $ /path/to/plugins/checks/cloudflare/dns.py remove test@example.com apikey123 example.com --content  10 .0.0.1", 
            "title": "Plugin Arguments"
        }, 
        {
            "location": "/facts/", 
            "text": "Automatron leverages the power of \nJinja2\n, a popular Python based templating language to enhance how runbooks can be used. The below example is a runbook that leverages Jinja2.\n\n\nname\n:\n \nCheck NGINX\n\n\n{%\n \nif\n \nprod\n \nin\n \nfacts\n[\nhostname\n]\n \n%}\n\n\nschedule\n:\n\n  \nsecond\n:\n \n*\n/20\n\n\n{%\n \nelse\n \n%}\n\n\nschedule\n:\n \n*/2\n \n*\n \n*\n \n*\n \n*\n\n\n{%\n \nendif\n \n%}\n\n\nchecks\n:\n\n  \nnginx_is_running\n:\n\n    \nexecute_from\n:\n \ntarget\n\n    \ntype\n:\n \ncmd\n\n    \ncmd\n:\n \nservice nginx status\n\n  \nport_443_is_up\n:\n\n    \nexecute_from\n:\n \ntarget\n\n    \ntype\n:\n \nplugin\n\n    \nplugin\n:\n \nnetwork/tcp_connect.py\n\n    \nargs\n:\n \n--host=\n{{\n \nfacts\n[\nnetwork\n][\neth0\n][\nv4\n][\n0\n]\n \n}}\n --port 443\n\n\nactions\n:\n\n  \nrestart_nginx\n:\n\n    \nexecute_from\n:\n \ntarget\n\n    \ntrigger\n:\n \n2\n\n    \nfrequency\n:\n \n300\n\n    \ncall_on\n:\n\n      \n-\n \nWARNING\n\n      \n-\n \nCRITICAL\n\n      \n-\n \nUNKNOWN\n\n    \ntype\n:\n \ncmd\n\n    \ncmd\n:\n \nservice nginx restart\n\n  \nremove_from_dns\n:\n\n    \nexecute_from\n:\n \nremote\n\n    \ntrigger\n:\n \n0\n\n    \nfrequency\n:\n \n0\n\n    \ncall_on\n:\n\n      \n-\n \nWARNING\n\n      \n-\n \nCRITICAL\n\n      \n-\n \nUNKNOWN\n\n    \ntype\n:\n \nplugin\n\n    \nplugin\n:\n \ncloudflare/dns.py\n\n    \nargs\n:\n \nremove test@example.com apikey123 example.com --content\n \n{{\n \nfacts\n[\nnetwork\n][\neth0\n][\nv4\n][\n0\n]\n \n}}\n\n\n\n\n\n\nThe above runbook leverages both Jinja2 and Automatron's internal \nFacts\n. Facts are variables that Automatron has collected during the Vetting process of each monitored system.\n\n\nWhen Automatron discovers a new host it executes Vetting Plugins on the host. Some plugins are executed remotely, others are executed on the host itself. These plugins return information unique to each host.\n\n\nAn example of the type of information can be seen in the \nontarget/system_info.py\n vetting plugin. This plugin creates facts for OS Distribution, Hostname, Kernel version and Network Information.\n\n\nA Deeper Look\n\n\nTo get a better understanding of facts, and how they can be used let's look at the facts used in the above example. The below example is an example of using the \nhostname\n fact to determine if the target is a \"production\" hostname or not.\n\n\n{%\n \nif\n \nprod\n \nin\n \nfacts\n[\nhostname\n]\n \n%}\n\n\nschedule\n:\n\n  \nsecond\n:\n \n*\n/20\n\n\n{%\n \nelse\n \n%}\n\n\nschedule\n:\n \n*/2\n \n*\n \n*\n \n*\n \n*\n\n\n{%\n \nendif\n \n%}\n\n\n\n\n\n\nThis next example uses another fact to determine the IPv4 address of the monitors host. This address is then used as an argument for the \ntcp_connect.py\n plugin.\n\n\nport_443_is_up\n:\n\n  \nexecute_from\n:\n \ntarget\n\n  \ntype\n:\n \nplugin\n\n  \nplugin\n:\n \nnetwork/tcp_connect.py\n\n  \nargs\n:\n \n--host=\n{{\n \nfacts\n[\nnetwork\n][\neth0\n][\nv4\n][\n0\n]\n \n}}\n --port 443\n\n\n\n\n\n\nThe above are simple examples of how Jinja and Facts used together can enable the creation of runbooks that can span multiple hosts and use cases.", 
            "title": "Facts"
        }, 
        {
            "location": "/facts/#a-deeper-look", 
            "text": "To get a better understanding of facts, and how they can be used let's look at the facts used in the above example. The below example is an example of using the  hostname  fact to determine if the target is a \"production\" hostname or not.  {%   if   prod   in   facts [ hostname ]   %}  schedule : \n   second :   * /20  {%   else   %}  schedule :   */2   *   *   *   *  {%   endif   %}   This next example uses another fact to determine the IPv4 address of the monitors host. This address is then used as an argument for the  tcp_connect.py  plugin.  port_443_is_up : \n   execute_from :   target \n   type :   plugin \n   plugin :   network/tcp_connect.py \n   args :   --host= {{   facts [ network ][ eth0 ][ v4 ][ 0 ]   }}  --port 443   The above are simple examples of how Jinja and Facts used together can enable the creation of runbooks that can span multiple hosts and use cases.", 
            "title": "A Deeper Look"
        }, 
        {
            "location": "/plugins/checks/", 
            "text": "While Automatron tries to make runbooks as simple as executing shell commands there is sometimes a need to take things a bit further than a simple one-liner.\n\n\nTo facilitate this Automatron also enables users to create custom health check plugins. These plugins are Nagios compatible executables that can be written a programming language of your choice.\n\n\nA simple example\n\n\nIn this walkthrough we will go ahead and create a custom health check plugin that pings a remote host to determine its availability. To get started, let's take a look at a runbook example of this.\n\n\nname\n:\n \nPing Check\n\n\nschedule\n:\n \n*/2\n \n*\n \n*\n \n*\n \n*\n\n\nchecks\n:\n\n  \nhost_is_pingable\n:\n\n    \nexecute_from\n:\n \ntarget\n\n    \ntype\n:\n \nplugin\n\n    \nplugin\n:\n \nping/check.sh\n\n    \nargs\n:\n \n10.0.0.1\n\n\n\n\n\n\nThis runbook uses the plugin system to copy the \nping/check.sh\n script to the target host and executes it with the arguments of \n10.0.0.1\n. This is equivalent to the following command.\n\n\n$ ping/check.sh \n10\n.0.0.1\n\n\n\n\n\nWith the runbook understood, let's now start writing our plugin.\n\n\nPlugin code\n\n\nThis type of health check plugin is very simple to write. The following \nBASH\n script is a working example that could easily be used with Automatron.\n\n\n#!/bin/bash\n\n\nif\n \n[\n -z \n$1\n \n]\n\n\nthen\n\n    \nexit\n \n1\n\n\nfi\n\n\n/bin/ping -c \n1\n \n$1\n \n /dev/null \n2\n1\n\n\nif\n \n[\n \n$?\n -eq \n0\n \n]\n\n\nthen\n\n    \necho\n \nPING OK UP\n\n    \nexit\n \n0\n\n\nelse\n\n    \necho\n \nPING CRITICAL DOWN\n\n    \nexit\n \n2\n\n\nfi\n\n\n\n\n\n\nThis script follows the \nNagios Plugin API\n specifications however the most important aspect of this health check is the exit code used. Automatron follows the same exit code mapping used by Nagios, as such the below list shows the mapping of exit code to health check status.\n\n\n\n\nOK\n: Requires a successful exit code of \n0\n\n\nWARNING\n: Is indicated by an exit code of \n1\n\n\nCRITICAL\n: Is indicated by an exit code of \n2\n\n\nUNKNOWN\n: Is indicated by any other exit code\n\n\n\n\n\n\nTip\n\n\nSince Automatron supports Nagios compliant scripts you can simply use existing \nNagios Plugins\n and other plugins from any monitoring solution that follows the Nagios plugin model.\n\n\nSome additional sources are: \nIcinga Exchange\n, \nSensu Plugins\n, and \nMonitoring Plugins Project\n.\n\n\n\n\n\n\nInfo\n\n\nAt this time Automatron ships with limited plugins available, in addition to the resources above additional Automatron plugins can be found at the \nAutomatron Plugins\n project.", 
            "title": "Checks"
        }, 
        {
            "location": "/plugins/checks/#a-simple-example", 
            "text": "In this walkthrough we will go ahead and create a custom health check plugin that pings a remote host to determine its availability. To get started, let's take a look at a runbook example of this.  name :   Ping Check  schedule :   */2   *   *   *   *  checks : \n   host_is_pingable : \n     execute_from :   target \n     type :   plugin \n     plugin :   ping/check.sh \n     args :   10.0.0.1   This runbook uses the plugin system to copy the  ping/check.sh  script to the target host and executes it with the arguments of  10.0.0.1 . This is equivalent to the following command.  $ ping/check.sh  10 .0.0.1  With the runbook understood, let's now start writing our plugin.", 
            "title": "A simple example"
        }, 
        {
            "location": "/plugins/checks/#plugin-code", 
            "text": "This type of health check plugin is very simple to write. The following  BASH  script is a working example that could easily be used with Automatron.  #!/bin/bash  if   [  -z  $1   ]  then \n     exit   1  fi \n\n/bin/ping -c  1   $1    /dev/null  2 1  if   [   $?  -eq  0   ]  then \n     echo   PING OK UP \n     exit   0  else \n     echo   PING CRITICAL DOWN \n     exit   2  fi   This script follows the  Nagios Plugin API  specifications however the most important aspect of this health check is the exit code used. Automatron follows the same exit code mapping used by Nagios, as such the below list shows the mapping of exit code to health check status.   OK : Requires a successful exit code of  0  WARNING : Is indicated by an exit code of  1  CRITICAL : Is indicated by an exit code of  2  UNKNOWN : Is indicated by any other exit code    Tip  Since Automatron supports Nagios compliant scripts you can simply use existing  Nagios Plugins  and other plugins from any monitoring solution that follows the Nagios plugin model.  Some additional sources are:  Icinga Exchange ,  Sensu Plugins , and  Monitoring Plugins Project .    Info  At this time Automatron ships with limited plugins available, in addition to the resources above additional Automatron plugins can be found at the  Automatron Plugins  project.", 
            "title": "Plugin code"
        }, 
        {
            "location": "/plugins/actions/", 
            "text": "Actions can be as simple as a one-liner shell command or as complicated as a 1,000 line Python script. With Automatron's action plugins both of these scenarios can be covered.\n\n\nAutomatron action plugins are simply executables that are copied and executed on the specified target. There is no special language or coding syntax required other than the fact that the script should be executable without specifying the language.\n\n\nA simple example\n\n\nTo understand this further let's write a simple plugin that finds files that require a \ndos2unix\n conversion and converts them. Before getting into our script, let's start with a runbook for this example.\n\n\nname\n:\n \nCheck if application has started\n\n\nschedule\n:\n\n  \nsecond\n:\n \n*/30\n\n\nchecks\n:\n\n  \nport_443_is_up\n:\n\n    \nexecute_from\n:\n \ntarget\n\n    \ntype\n:\n \nplugin\n\n    \nplugin\n:\n \nnetwork/tcp_connect.py\n\n    \nargs\n:\n \n--host=localhost --port 443\n\n\nactions\n:\n\n  \ndos2unix_files\n:\n\n    \nexecute_from\n:\n \ntarget\n\n    \ntrigger\n:\n \n0\n\n    \nfrequency\n:\n \n900\n\n    \ncall_on\n:\n\n      \n-\n \nWARNING\n\n      \n-\n \nCRITICAL\n\n      \n-\n \nUNKNOWN\n\n    \ntype\n:\n \nplugin\n\n    \nplugin\n:\n \ncustom/dos2unix.sh\n\n    \nargs\n:\n \n/application/path\n\n\n\n\n\n\nThe above runbook will verify if the application is listening on port 443 and if not execute the \ndos2unix_files\n action. This action is a plugin action which will copy the \nplugins/actions/custom/dos2unix.sh\n script to the target server and execute it with the argument of \n/application/path\n. To explain this better, the below command represents how this plugin would be executed on the target server.\n\n\n$ custom/dos2unix.sh /application/path\n\n\n\n\n\nThis script itself can be very simple and only needs to accept the arguments passed.\n\n\nSample code\n\n\nThe below script is a sample script that will perform a \ndos2unix\n conversion on any files within the specified path.\n\n\n#!/bin/bash\n\n\n\nif\n \n[\n -z \n$1\n \n]\n\n\nthen\n\n  \necho\n \nUsage: \n$0\n /path\n\n  \nexit\n \n1\n\n\nfi\n\n\n\nBEFORE\n=\n$(\nfind \n$1\n -type f -exec file \n{}\n \n\\;\n \n|\n grep -c \nwith CRLF line terminators\n)\n\n\n\nfor\n FILE in \n`\nfind \n$1\n -type f -exec file \n{}\n \n\\;\n \n|\n grep \nwith CRLF line terminators\n \n|\n cut -d: -f1\n`\n\n\ndo\n\n  dos2unix \n$FILE\n\n\ndone\n\n\n\nAFTER\n=\n$(\nfind \n$1\n -type f -exec file \n{}\n \n\\;\n \n|\n grep -c \nwith CRLF line terminators\n)\n\n\n\nif\n \n[\n \n$AFTER\n -lt \n$BEFORE\n \n]\n\n\nthen\n\n  \nexit\n \n0\n\n\nelse\n\n  \nexit\n \n1\n\n\nfi\n\n\n\n\n\n\nThe exit codes used by the action plugin will be used by Automatron to determine successful action execution or a failed action execution. A script that is successful should exit with a code of \n0\n. Any other exit code will show as a failed action execution.\n\n\n\n\nInfo\n\n\nAutomatron action plugins are designed to be as simple as possible to allow users to use their existing scripts along with Automatron's automated action execution. Aside from appropriate exit code usage there is no coding required to use an existing script or utility.", 
            "title": "Actions"
        }, 
        {
            "location": "/plugins/actions/#a-simple-example", 
            "text": "To understand this further let's write a simple plugin that finds files that require a  dos2unix  conversion and converts them. Before getting into our script, let's start with a runbook for this example.  name :   Check if application has started  schedule : \n   second :   */30  checks : \n   port_443_is_up : \n     execute_from :   target \n     type :   plugin \n     plugin :   network/tcp_connect.py \n     args :   --host=localhost --port 443  actions : \n   dos2unix_files : \n     execute_from :   target \n     trigger :   0 \n     frequency :   900 \n     call_on : \n       -   WARNING \n       -   CRITICAL \n       -   UNKNOWN \n     type :   plugin \n     plugin :   custom/dos2unix.sh \n     args :   /application/path   The above runbook will verify if the application is listening on port 443 and if not execute the  dos2unix_files  action. This action is a plugin action which will copy the  plugins/actions/custom/dos2unix.sh  script to the target server and execute it with the argument of  /application/path . To explain this better, the below command represents how this plugin would be executed on the target server.  $ custom/dos2unix.sh /application/path  This script itself can be very simple and only needs to accept the arguments passed.", 
            "title": "A simple example"
        }, 
        {
            "location": "/plugins/actions/#sample-code", 
            "text": "The below script is a sample script that will perform a  dos2unix  conversion on any files within the specified path.  #!/bin/bash  if   [  -z  $1   ]  then \n   echo   Usage:  $0  /path \n   exit   1  fi  BEFORE = $( find  $1  -type f -exec file  {}   \\;   |  grep -c  with CRLF line terminators )  for  FILE in  ` find  $1  -type f -exec file  {}   \\;   |  grep  with CRLF line terminators   |  cut -d: -f1 `  do \n  dos2unix  $FILE  done  AFTER = $( find  $1  -type f -exec file  {}   \\;   |  grep -c  with CRLF line terminators )  if   [   $AFTER  -lt  $BEFORE   ]  then \n   exit   0  else \n   exit   1  fi   The exit codes used by the action plugin will be used by Automatron to determine successful action execution or a failed action execution. A script that is successful should exit with a code of  0 . Any other exit code will show as a failed action execution.   Info  Automatron action plugins are designed to be as simple as possible to allow users to use their existing scripts along with Automatron's automated action execution. Aside from appropriate exit code usage there is no coding required to use an existing script or utility.", 
            "title": "Sample code"
        }, 
        {
            "location": "/plugins/discovery/", 
            "text": "Automated host discovery is one of the key functions of Automatron. Like the majority of Automatron this functionality can be extended through a serious of plugins. However, unlike the \nchecks\n or \nactions\n plugins the \ndiscovery\n plugins are a bit more complex.\n\n\nTo get started it is highly recommended to reference an existing plugin such as the \nAWS Discovery\n plugin.\n\n\nTo help understand the Discovery plugins, let's take a look at a simple example. The below is a copy of the \nroster\n discovery plugin.\n\n\n Roster file discovery plugin \n\n\n\nimport\n \ntime\n\n\nimport\n \njson\n\n\nimport\n \nrequests\n\n\nfrom\n \ncore.discover\n \nimport\n \nBaseDiscover\n\n\nimport\n \ncore.logs\n\n\n\nclass\n \nDiscover\n(\nBaseDiscover\n):\n\n    \n Main Discover Class \n\n\n    \ndef\n \nstart\n(\nself\n):\n\n        \n Start Discovery \n\n        \nlogs\n \n=\n \ncore\n.\nlogs\n.\nLogger\n(\nconfig\n=\nself\n.\nconfig\n,\n \nproc_name\n=\ndiscovery.roster\n)\n\n        \nlogger\n \n=\n \nlogs\n.\ngetLogger\n()\n\n        \nlogger\n \n=\n \nlogs\n.\nclean_handlers\n(\nlogger\n)\n\n        \nlogger\n.\ndebug\n(\nGetting hosts from Roster\n)\n\n\n        \nwhile\n \nTrue\n:\n\n            \nfound\n \n=\n \n[]\n\n            \ntry\n:\n\n                \nfor\n \nip\n \nin\n \nself\n.\nconfig\n[\ndiscovery\n][\nplugins\n][\nroster\n][\nhosts\n]:\n\n                    \nfound\n.\nappend\n(\nip\n)\n\n                    \nif\n \nself\n.\ndbc\n.\nnew_discovery\n(\nip\n=\nip\n):\n\n                        \nlogger\n.\ndebug\n(\nAdded host {0} to discovery queue\n.\nformat\n(\nip\n))\n\n                    \nelse\n:\n\n                        \nlogger\n.\ndebug\n(\nFailed to add host {0} to discovery queue\n.\nformat\n(\nip\n))\n\n            \nexcept\n \nKeyError\n \nas\n \ne\n:\n\n                \nlogger\n.\nwarn\n(\nConfiguration syntax error: {0}\n.\nformat\n(\ne\n.\nmessage\n))\n\n\n            \nlogger\n.\ninfo\n(\nFound {0} hosts\n.\nformat\n(\nlen\n(\nfound\n)))\n\n            \nif\n \nunit_testing\n \nin\n \nself\n.\nconfig\n.\nkeys\n():\n\n                \n# Break out of loop for unit testing\n\n                \nbreak\n\n            \nelse\n:\n\n                \n# Adding sleep() so master process doesn\nt exit after completion\n\n                \ntime\n.\nsleep\n(\n900\n)\n\n        \n# Return true for unit testing\n\n        \nreturn\n \nTrue\n\n\n\n\n\n\nThe above plugin is written in Python and has some basic requirements.\n\n\nThe first requirement is that the \nclass\n name is always \nDiscover\n. This is the class that will be dynamically loaded by Automatron, if this name is incorrect the plugin will not work.\n\n\nThe second requirement is that the plugin must add newly discovered target hosts via the \nself.dbc.new_discovery()\n function shown above. This function will add the target host to Automatron's internal queue for newly discovered hosts.", 
            "title": "Discovery"
        }, 
        {
            "location": "/plugins/vetting/", 
            "text": "Vetting plugins are used during the host discovery process to identify \nfacts\n for target systems. These are simple executables that live in either \nplugins/vetting/ontarget\n or \nplugins/vetting/remote\n. The vetting plugins located within the \nontarget\n directory are copied to the monitored host and executed. Plugins located within the \nremote\n directory are executed remotely from the Automatron instance.\n\n\nMuch like other plugins within Automatron these plugins are language agnostic. The only requirement for these executables is that they return a JSON structure with the identified facts specified.\n\n\nThe below example shows a simple \nping\n status vetting plugin.\n\n\n#!/usr/bin/env bash\n\n\n## Ping server then return true or false JSON\n\n\n\nif\n \n[\n -z \n$1\n \n]\n\n\nthen\n\n    \nexit\n \n1\n\n\nfi\n\n\n/bin/ping -c \n1\n \n$1\n \n /dev/null \n2\n1\n\n\nif\n \n[\n \n$?\n -eq \n0\n \n]\n\n\nthen\n\n    \necho\n \n{\nping\n: true}\n\n\nelse\n\n    \necho\n \n{\nping\n: false}\n\n\nfi\n\n\n\n\n\n\nThis plugin is designed to live within the \nplugins/vetting/remote\n path, as such it accepts a single argument \n$1\n which will be either the IP or Hostname of the desired target. All plugins in the \nremote\n directory are executed with this single argument, whereas plugins in the \nontarget\n path are executed with no arguments.\n\n\nThis plugin will produce a single fact that is accessible as \n{{ facts['ping'] }}\n.\n\n\n\n\nTip\n\n\nAnother, more complex example vetting plugin can be found with the \nsystem_info.py\n plugin.", 
            "title": "Vetting"
        }
    ]
}